# -*- coding: utf-8 -*-
"""Copy of Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18K4cKep0G4JvjLiYVZ0gYErbAoYM0rW1
"""

# install library newsapi dan NLTK
!pip install newsapi-python
!pip install nltk

import numpy as np
import pandas as pd
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from newsapi import NewsApiClient
import re
from tqdm import tqdm

nltk.download('punkt_tab')
nltk.download('wordnet')
nltk.download('omw-1.4')

class TextProcessor:
    def __init__(self):
        self.lemmatizer = WordNetLemmatizer()

    def cleanse(self, text):
        text = text.lower()
        text = re.sub(r'\[\+\d+\schars\]', '', text).strip()
        text = re.sub(r'[^a-zA-Z0-9\s]', '', text)
        tokens = word_tokenize(text)
        lemmas = [self.lemmatizer.lemmatize(token) for token in tokens]
        return ' '.join(lemmas)

    def tokenize(self, text):
        return word_tokenize(text)


class SkipGramModel:
    def __init__(self, vocab_size, embedding_dim):
        self.vocab_size = vocab_size
        self.embedding_dim = embedding_dim
        self.W1 = np.random.randn(vocab_size, embedding_dim)
        self.W2 = np.random.randn(embedding_dim, vocab_size)

    def forward(self, one_hot_vector):
        hidden_layer = np.dot(one_hot_vector, self.W1)
        output_layer = self._softmax(np.dot(hidden_layer, self.W2))
        return hidden_layer, output_layer

    def backward(self, one_hot_vector, target_vector, learning_rate=0.01):
        hidden_layer, output_layer = self.forward(one_hot_vector)
        error = target_vector - output_layer
        output_gradient = np.outer(hidden_layer, error)
        hidden_gradient = np.outer(one_hot_vector, np.dot(self.W2, error))
        self.W1 -= learning_rate * hidden_gradient
        self.W2 -= learning_rate * output_gradient

    def _softmax(self, x):
        exp_x = np.exp(x - np.max(x))
        return exp_x / exp_x.sum()


class SkipGramTrainer:
    def __init__(self, window_size=2, embedding_dim=20, epochs=100):
        self.window_size = window_size
        self.embedding_dim = embedding_dim
        self.epochs = epochs

    def generate_skip_grams(self, tokens):
        skip_grams = []
        for idx, word in enumerate(tokens):
            for offset in range(-self.window_size, self.window_size + 1):
                context_idx = idx + offset
                if 0 <= context_idx < len(tokens) and idx != context_idx:
                    skip_grams.append((word, tokens[context_idx]))
        return skip_grams

    def train(self, sentences):
        vocab = [word for sentence in sentences for word in sentence]
        word2idx = {word: idx for idx, word in enumerate(set(vocab))}
        training_pairs = self.generate_skip_grams(vocab)
        model = SkipGramModel(vocab_size=len(word2idx), embedding_dim=self.embedding_dim)

        history_loss = []
        for _ in tqdm(range(self.epochs)):
            total_loss = 0
            for target, context in training_pairs:
                target_vector = np.zeros(len(word2idx))
                context_vector = np.zeros(len(word2idx))
                target_vector[word2idx[target]] = 1
                context_vector[word2idx[context]] = 1
                hidden, output = model.forward(target_vector)
                loss = -np.log(output[word2idx[context]])
                model.backward(target_vector, context_vector)
                total_loss += loss
            history_loss.append(total_loss / len(training_pairs))

        return model, word2idx, history_loss


class NewsScraper:
    def __init__(self, api_key):
        self.newsapi = NewsApiClient(api_key=api_key)

    def scrape(self, query, from_param, to, language='en', sort_by='popularity', max_page=1):
        df_all = pd.DataFrame()
        for page in range(1, max_page + 1):
            articles = self.newsapi.get_everything(q=query, from_param=from_param, to=to,
                                                   language=language, sort_by=sort_by, page=page)
            df_tmp = pd.json_normalize(articles['articles'])
            df_all = pd.concat([df_all, df_tmp], axis=0).reset_index(drop=True)
        return df_all.drop_duplicates()


class SentenceSimilarity:
    @staticmethod
    def cosine_similarity(v1, v2):
        dot_product = np.dot(v1, v2)
        magnitude = np.linalg.norm(v1) * np.linalg.norm(v2)
        return dot_product / magnitude if magnitude else 0

    @staticmethod
    def get_embedding(sentence, model, word2idx, processor):
        tokens = processor.tokenize(processor.cleanse(sentence))
        embeddings = [model.W1[word2idx[token]] for token in tokens if token in word2idx]
        return np.mean(embeddings, axis=0) if embeddings else np.zeros(model.embedding_dim)

if __name__ == '__main__':
    YOUR_API_KEY = '1a13883e32e54d14915ebe98f11270e5'
    scraper = NewsScraper(YOUR_API_KEY)
    processor = TextProcessor()

    df_news = scraper.scrape('bitcoin', '2025-02-14', '2025-03-14', max_page=5)
    cleaned_sentences = df_news['content'].astype(str).apply(processor.cleanse).head(10)
    tokenized_sentences = [processor.tokenize(sentence) for sentence in cleaned_sentences]

    trainer = SkipGramTrainer(window_size=3, embedding_dim=50, epochs=100)
    model, word2idx, history_loss = trainer.train(tokenized_sentences)

    s1 = "Donald Trump delivered a speech at CPAC wearing his trademark sunglasses"
    s2 = "A document obtained by Wired showed a dramatic speech made by Donald Trump at CPAC"

    s1_emb = SentenceSimilarity.get_embedding(s1, model, word2idx, processor)
    s2_emb = SentenceSimilarity.get_embedding(s2, model, word2idx, processor)

    similarity = SentenceSimilarity.cosine_similarity(s1_emb, s2_emb)
    print(f"Cosine Similarity between sentences: {similarity:.2f}")

